import numpy as np
import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split  
from sklearn.metrics import accuracy_score 
from sklearn.metrics import confusion_matrix
from sklearn. metrics import classification_report, roc_auc_score, roc_curve
import pickle
import regex
import re
import streamlit as st
import matplotlib.pyplot as plt
from sklearn import metrics
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.tokenize import sent_tokenize
from underthesea import word_tokenize, pos_tag, sent_tokenize
from wordcloud import WordCloud


nltk.download('punkt')


#Text Processing
##LOAD EMOJICON
with open('emojicon.txt', 'r', encoding="utf8") as file:  
    emoji_lst = file.read().split('\n')
    emoji_dict = {}
    for line in emoji_lst:
        key, value = line.split('\t')
        emoji_dict[key] = str(value)

#################
#LOAD TEENCODE
with open('teencode.txt', 'r', encoding="utf8") as file:  
    teen_lst = file.read().split('\n')
    teen_dict = {}
    for line in teen_lst:
        key, value = line.split('\t')
        teen_dict[key] = str(value)

###############
#LOAD TRANSLATE ENGLISH -> VNMESE
with open('english-vnmese.txt', 'r', encoding="utf8") as file:  
    english_lst = file.read().split('\n')
    english_dict = {}
    for line in english_lst:
        key, value = line.split('\t')
        english_dict[key] = str(value)

################
#LOAD wrong words
with open('wrong-word.txt', 'r', encoding="utf8") as file:  
    wrong_lst = file.read().split('\n')

#################
#LOAD STOPWORDS
with open('vietnamese-stopwords.txt', 'r', encoding="utf8") as file:  
    stopwords_lst = file.read().split('\n')
  

def process_text_str(text, emoji_dict, teen_dict, wrong_lst):
    document = text.lower()
    document = document.replace("â€™",'')
    document = regex.sub(r'\.+', ".", document)
    new_sentence =''
    for sentence in sent_tokenize(document):
        # if not(sentence.isascii()):
        ###### CONVERT EMOJICON
        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))
        ###### CONVERT TEENCODE
        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())
        ###### DEL Punctuation & Numbers
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(re.findall(pattern,sentence))
        # ...
        ###### DEL wrong words
        sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())
        new_sentence = new_sentence+ sentence + '. '
    document = new_sentence
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    #...
    return document

def process_text(text, emoji_dict, teen_dict, wrong_lst):
    if isinstance(text, float):
        text = str(text)
    document = text.lower()
    document = document.replace("â€™",'')
    document = regex.sub(r'\.+', ".", document)
    new_sentence =''
    for sentence in sent_tokenize(document):
        # if not(sentence.isascii()):
        ###### CONVERT EMOJICON
        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))
        ###### CONVERT TEENCODE
        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())
        ###### DEL Punctuation & Numbers
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(regex.findall(pattern,sentence))
        # ...
        ###### DEL wrong words
        sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())
        new_sentence = new_sentence+ sentence + '. '
    document = new_sentence
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    #...
    return document



# Chuáº©n hÃ³a unicode tiáº¿ng viá»‡t
def loaddicchar():
    uniChars = "Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬Ä‚áº°áº®áº²áº´áº¶ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»’á»á»”á»–á»˜Æ á»œá»šá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»ªá»¨á»¬á»®á»°á»²Ãá»¶á»¸á»´Ã‚Ä‚ÄÃ”Æ Æ¯"
    unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"

    dic = {}
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(
        '|')
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic

# ÄÆ°a toÃ n bá»™ dá»¯ liá»‡u qua hÃ m nÃ y Ä‘á»ƒ chuáº©n hÃ³a láº¡i
def convert_unicode(txt):
    dicchar = loaddicchar()
    return regex.sub(
        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
        lambda x: dicchar[x.group()], txt)



def process_special_word(text):
    # cÃ³ thá»ƒ cÃ³ nhiá»u tá»« Ä‘áº·c biá»‡t cáº§n rÃ¡p láº¡i vá»›i nhau
    new_text = ''
    text_lst = text.split()
    i= 0
    # khÃ´ng, cháº³ng, cháº£...
    if 'khÃ´ng' in text_lst:
        while i <= len(text_lst) - 1:
            word = text_lst[i]
            #print(word)
            #print(i)
            if  word == 'khÃ´ng':
                next_idx = i+1
                if next_idx <= len(text_lst) -1:
                    word = word +'_'+ text_lst[next_idx]
                i= next_idx + 1
            else:
                i = i+1
            new_text = new_text + word + ' '
    else:
        new_text = text
    return new_text.strip()



import re
# HÃ m Ä‘á»ƒ chuáº©n hÃ³a cÃ¡c tá»« cÃ³ kÃ½ tá»± láº·p
def normalize_repeated_characters(text):
    # Thay tháº¿ má»i kÃ½ tá»± láº·p liÃªn tiáº¿p báº±ng má»™t kÃ½ tá»± Ä‘Ã³
    # VÃ­ dá»¥: "ngonnnn" thÃ nh "ngon", "thiá»‡tttt" thÃ nh "thiá»‡t"
    return re.sub(r'(.)\1+', r'\1', text)

# Ãp dá»¥ng hÃ m chuáº©n hÃ³a cho vÄƒn báº£n
# print(normalize_repeated_characters(example))



def process_postag_thesea(text):
    new_document = ''
    for sentence in sent_tokenize(text):
        sentence = sentence.replace('.','')
        ###### POS tag
        lst_word_type = ['N','Np','A','AB','V','VB','VY','R']
        # lst_word_type = ['A','AB','V','VB','VY','R']
        sentence = ' '.join( word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format="text"))))
        new_document = new_document + sentence + ' '
    ###### DEL excess blank space
    new_document = regex.sub(r'\s+', ' ', new_document).strip()
    return new_document


stop_words = [
    'nhÃ  hÃ ng', 'nhÃ _hÃ ng', 'quÃ¡n', 'quÃ¡n_Äƒn', 'Äƒn', 'Ä‘á»“_Äƒn', 'bá»¯a', 'buá»•i', 'trÆ°a', 'tá»‘i', 'hÃ´m nay', 'ngÃ y mai'
    'sÃ¡ng', 'thá»±c_Ä‘Æ¡n', 'mÃ³n', 'mÃ³n_Äƒn', 'bÃ n', 'Ä‘áº·t bÃ n', 'Ä‘áº·t_bÃ n', 'nhÃ¢n_viÃªn', 'phá»¥c_vá»¥', 'dá»‹ch_vá»¥',
    'khÃ¡ch_hÃ ng', 'khÃ¡ch', 'Ä‘á»“_uá»‘ng', 'mÃ³n', 'mÃ³n_Äƒn' 'giÃ¡', 'hÃ³a_Ä‘Æ¡n',
    'pháº£i_chÄƒng', 'khÃ´ng_gian', 'trang_trÃ­', 'chá»—', 'vá»‹_trÃ­', 'khu_vá»±c', 'cháº¥t_lÆ°á»£ng', 'sá»‘_lÆ°á»£ng',
    'Ä‘á»_nghá»‹', 'gá»£i_Ã½', 'tráº£i_nghiá»‡m', 'thá»­', 'thÆ°á»Ÿng_thá»©c', 'Ä‘Ã¡nh_giÃ¡', 'sao', 'lÃ ', 'luÃ´n',  'cÃ³',
    'ná»¯a', 'nÃ³i', 'tháº¥y', 'quÃ¡', 'cÅ©ng', 'lÃ m', 'cÃ²n', 'bÃ¡nh','ngÆ°á»i', 'thÃªm', 'khÃ¡c', 'báº¡n', 'láº¡i', 'nhÃ¬n',
    'pháº§n', 'gá»i', 'bÃªn', 'chá»‰', 'lÃªn', 'gÃ ', 'bÃ¡n', 'cháº¯c', 'pháº£i',  'lÃºc', 'Ä‘i', 'kiá»ƒu', 'cÆ¡m', 'Ä‘áº·t', 'vá»',
    'chÆ°a', 'kÃªu',  'mÃ¬', 'bÃ¡nh_mÃ¬', 'hÃ´m', 'thá»‹t', 'tÃ´', 'Ä‘áº¿n', 'hÃ ng', 'nÆ°á»›c cháº¥m', 'tÃ­nh', 'nÆ°á»›c dÃ¹ng', 'ngá»“i',
    'nÆ°á»›c lÃ¨o', 'láº§n Ä‘áº§u', 'láº¥y', 'há»™p', 'kÃ¨m', 'cá»©', 'tiá»‡m', 'nhÃ ', 'tá»›i', 'bÃ²', 'sá»‘t', 'Ä‘á»“', 'bá»', 'cÆ¡m gÃ ', 'chá»§',
    'cÅ©ng ráº¥t', 'ngÃ y', 'thÆ°á»ng', 'cÃ²n cÃ³', 'giá»‘ng', 'cháº£', 'sÃ¡ng', 'ghÃ©', 'tháº¥y cÅ©ng', 'thá»©', 'chá»n', 'toÃ n', 'cÃ³ thÃªm',
    'vÃ o', 'riÃªng', 'Ä‘em', 'giÃ¡ cÅ©ng', 'há»i', 'sáº½', 'loáº¡i', 'vÃ´', 'Äƒn_á»Ÿ', 'cÃ¡ch', 'phÃ´', 'thá»©c_Äƒn', 'cháº¡y', 'giá»¯', 'chÃ¡o',
    'phá»Ÿ', 'Ä‘ang', 'bÃºn', 'tÃ´m', 'tháº¥y cÃ³', 'á»‘c', 'thá»‹t bÃ²', 'dÄ©a', 'cho', 'gá»i pháº§n', 'cá»±c', 'cÃ²n láº¡i','lÃºc_nÃ o cÅ©ng',
    'Ä‘i ngÆ°á»i', 'uá»‘ng', 'quáº­n', 'xÃ´i', 'chÃ¨', 'váº«n cÃ²n', 'nhÆ°_váº­y', 'má»Ÿ', 'báº£o', 'cÃ¹ng', 'Ä‘Æ°a', 'vá»‹t', 'ráº¥t lÃ ', 'nÆ°á»›c_máº¯m',
    'lÃ  tháº¥y', 'Ä‘Æ°á»ng', 'pháº§n cÆ¡m', 'gá»­i', 'táº§m', 'máº·t', 'trÆ°á»›c', 'Ä‘Æ°á»£c','láº¯m', 'ráº¥t', 'giÃ¡', 'khÃ¡', 'hÆ¡n', 'váº«n', 'háº¿t', 'láº§n', 'má»›i', 'khÃ´ng_cÃ³',
    'cÃ³_thá»ƒ', 'giá»', 'Ä‘á»u', 'biáº¿t', 'Ä‘Ãºng', 'khÃ´ng_biáº¿t', 'khÃ´ng_pháº£i', 'nÆ°á»›ng', 'hÆ¡i', 'nhiá»u láº§n',
   'láº§n Ä‘áº§u', 'nghÄ©', 'chiÃªn', 'Ä‘á»§', 'nhÃ¡nh', 'ngoÃ i', 'cÃ¡', 'Ä‘iá»ƒm', 'nhÆ°ng_mÃ ', 'hÃ¬nh', 'dá»‹p', 'nÆ¡i', 'chiá»u','trÃªn', 'trá»™n', 'cáº£m_giÃ¡c',
   'liá»n', 'hÃ¬nh_nhÆ°', 'miáº¿ng', 'náº¥u', 'tá»«ng', 'náº±m', 'sáºµn', 'sá»‘', 'máº¥t', 'nhá»›', 'chÃ©n', 'khoáº£ng', 'láº§n láº§n', 'khÃ´ng_tháº¥y',
   'Ä‘á»•i', 'cáº§n', 'máº¹', 'á»•', 'nháº­n', 'gá»“m', 'láº§n Ä‘áº§u_tiÃªn', 'khá»i', 'tÃ­', 'khÃ´ng', 'nhiÃªn', 'máº·c_dÃ¹', 'giÃ²', 'Ã¡', 'Ä‘áº§u',
   'nháº­n Ä‘Æ°á»£c', 'trá»i', 'giáº£m', 'viá»‡c', 'cá»±c_kÃ¬', 'tiáº¿p', 'Ä‘á»£i', 'rÃ¡n', 'lÃºc_nÃ o',  'cÃ³_Ä‘iá»u', 'láº§u', 'sá»£i', 'cháº³ng', 'cuá»‘n', 'thÃ nh', 'xuá»‘ng',
   'review', 'há»“i', 'bá»‹ch', 'miá»‡ng', 'dÃ¹ng', 'Ä‘Ã¹i', 'tÃ¢y', 'khÃ´ng_bá»‹', 'tÃªn', 'cáº£m_nháº­n', 'nhÃ³m',
   'tráº£', 'gá»i', 'hÆ¡n nhiá»u', 'nÃªn', 'má»›i Ä‘Æ°á»£c']




def remove_stopword(text):
    ###### REMOVE stop words
    document = ' '.join('' if word in stop_words else word for word in text.split())
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    return document


def clean_text_df(text):
  clean_text = text.apply(lambda x: process_text(x, emoji_dict, teen_dict, wrong_lst))
  clean_text = clean_text.apply(convert_unicode)
  clean_text = clean_text.apply(process_special_word)
  clean_text = clean_text.apply(normalize_repeated_characters)
  clean_text = clean_text.apply(process_postag_thesea)
  clean_text = clean_text.apply(remove_stopword)
  return clean_text

def clean_text_str(text):
  clean_text = process_text_str(text, emoji_dict, teen_dict, wrong_lst)
  clean_text = convert_unicode(clean_text)
  clean_text = process_special_word(clean_text)
  clean_text = normalize_repeated_characters(clean_text)
  clean_text = process_postag_thesea(clean_text)
  clean_text = remove_stopword(clean_text)
  return clean_text

def predict_sentiment(text):
    return 'ğŸ˜Š' if text == 1 else 'ğŸ˜'  

# Upload file
data = pd.read_csv('data_sentiment.csv')


#load model
with open('restaurant_sentiment.pkl', 'rb') as file:  
    sentiment_model = pickle.load(file)
with open('model_tfidf.pkl', 'rb') as file:  
    tfidf_model = pickle.load(file)  


#GUI
st.set_page_config(page_title='Sentiment Analysis', page_icon='ğŸ“Š', layout="wide")

menu = ["Business Objective", "Data Review", "Model", "Sentiment Analysis", "Restaurant Information"]
choice = st.sidebar.selectbox('Menu', menu)
if choice == 'Business Objective': 
    st.title("Project 4: Nguyá»…n Thá»‹ Ngá»c TrÃ¢m - ÄÃ o MÃ¬nh TrÃ­")
    st.title("ğŸ™‚ğŸ˜ğŸ˜  Sentiment Analysis")
    st.subheader(" Sentiment Analysis lÃ  quÃ¡ trÃ¬nh phÃ¢n tÃ­ch, Ä‘Ã¡nh giÃ¡ quan Ä‘iá»ƒm cá»§a má»™t ngÆ°á»i vá» má»™t Ä‘á»‘i tÆ°á»£ng nÃ o Ä‘Ã³ (quan Ä‘iá»ƒm mang tÃ­nh tÃ­ch cá»±c, tiÃªu cá»±c, hay trung tÃ­nh,..). QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ thá»±c hiá»‡n báº±ng viá»‡c sá»­ dá»¥ng cÃ¡c táº­p luáº­t (rule-based), sá»­ dá»¥ng  Machine Learning hoáº·c phÆ°Æ¡ng phÃ¡p Hybrid (káº¿t há»£p hai  phÆ°Æ¡ng phÃ¡p trÃªn).")  
    st.subheader("Sentiment Analysis Ä‘Æ°á»£c á»©ng dá»¥ng nhiá»u trong thá»±c táº¿, Ä‘áº·c biá»‡t lÃ  trong hoáº¡t Ä‘á»™ng quáº£ng bÃ¡ kinh doanh. Viá»‡c phÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡ cá»§a ngÆ°á»i dÃ¹ng vá» má»™t sáº£n pháº©m xem há» Ä‘Ã¡nh giÃ¡ tiÃªu cá»±c, tÃ­ch cá»±c hoáº·c Ä‘Ã¡nh giÃ¡ cÃ¡c háº¡n cháº¿ cá»§a sáº£n pháº©m sáº½ giÃºp cÃ´ng ty nÃ¢ng cao cháº¥t lÆ°á»£ng sáº£n pháº©m vÃ  tÄƒng cÆ°á»ng hÃ¬nh áº£nh cá»§a cÃ´ng ty, cá»§ngcá»‘ sá»± hÃ i lÃ²ng cá»§a khÃ¡ch hÃ ng.")
    st.image('sentimentanalysishotelgeneric-2048x803-1.jpg')
    st.title("ğŸ½ï¸ Sentiment Analysis trong áº©m thá»±c")
    st.subheader(" Äá»ƒ lá»±a chá»n má»™t nhÃ  hÃ ng/quÃ¡n Äƒn má»›i chÃºng ta cÃ³ xu hÆ°á»›ng xem xÃ©t nhá»¯ng bÃ¬nh luáº­n tá»« nhá»¯ng ngÆ°á»i Ä‘Ã£ thÆ°á»Ÿng thá»©c Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh cÃ³ nÃªn thá»­ hay khÃ´ng? ")
    st.subheader(" XÃ¢y dá»±ng há»‡ thá»‘ng há»— trá»£ nhÃ  hÃ ng/quÃ¡n Äƒn phÃ¢n loáº¡i cÃ¡c pháº£n há»“i cá»§a khÃ¡ch hÃ ng  thÃ nh cÃ¡c nhÃ³m: tÃ­ch cá»±c, tiÃªu cá»±c, trung tÃ­nh  dá»±a trÃªn dá»¯ liá»‡u dáº¡ng vÄƒn báº£n.")
    st.subheader(" Tá»« nhá»¯ng Ä‘Ã¡nh giÃ¡ cá»§a khÃ¡ch hÃ ng, váº¥n Ä‘á» Ä‘Æ°á»£c Ä‘Æ°a ra lÃ  lÃ m sao Ä‘á»ƒ cÃ¡c nhÃ  hÃ ng/ quÃ¡n Äƒn hiá»ƒu Ä‘Æ°á»£c khÃ¡ch hÃ ng rÃµ hÆ¡n, biáº¿t há» Ä‘Ã¡nh giÃ¡ vá» mÃ¬nh nhÆ° tháº¿ nÃ o Ä‘á»ƒ cáº£i thiá»‡n hÆ¡n trong dá»‹ch vá»¥/ sáº£n pháº©m.")
    st.image('vn-11134513-7r98o-lugftthr8is27b.png')
    st.title("ğŸ‘©â€ğŸ’» CÃ¡c bÆ°á»›c thá»±c hiá»‡n")
    st.image('project-10.jpg')


elif choice == 'Data Review':
    data_review = pd.read_csv('data_review_merge.csv')
    restaurant = pd.read_csv('1_Restaurants.csv')
    st.title("ğŸ” Data Review ğŸ“")
    st.subheader('Dá»¯ liá»‡u Ä‘Æ°á»£c cung cáº¥p sáºµn trong táº­p tin 2_Reviews.csv vá»›i gáº§n 30.000 máº«u gá»“m cÃ¡c thÃ´ng tin:')
    st.subheader('ID (mÃ£), User (ngÆ°á»i dÃ¹ng), Time (thá»i gian Ä‘Ã¡nh giÃ¡), Rating (Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡), Comment (ná»™i dung Ä‘Ã¡nh giÃ¡), vÃ  IDRestaurant (mÃ£ nhÃ  hÃ ng)')
    st.subheader('Táº­p tin chá»©a thÃ´ng tin vá» nhÃ  hÃ ng: 1_Restaurants.csv vá»›i hÆ¡n 1.600 máº«u gá»“m cÃ¡c thÃ´ng tin:')
    st.subheader('ID (mÃ£), Restaurant (tÃªn nhÃ  hÃ ng), Address (Ä‘á»‹a chá»‰), Time (giá» má»Ÿ cá»­a), Price (khoáº£ng giÃ¡), District(quáº­n)')
    st.markdown("##")
    st.subheader(f"Tá»•ng sá»‘ lÆ°á»£ng nhÃ  hÃ ng: {len(list(restaurant['Restaurant'].value_counts().index))}")

    dis_res = restaurant.groupby('District')['Restaurant'].count().sort_values(ascending=False)
    top_10 = data_review.groupby('Restaurant')['Rating'].count().sort_values(ascending=False).head(10)
 
  
    st.subheader("Tá»•ng sá»‘ lÆ°á»£ng nhÃ  hÃ ng theo quáº­n")
    plt.figure(figsize=(10, 8))
    ax = dis_res.plot(kind='barh', x='District', y='Number of Restaurants', legend=False)
    ax.set_xlabel("Number of Restaurants")
    plt.title('Number of Restaurants by District')
    for container in ax.containers:
        ax.bar_label(container, label_type='edge')
    st.pyplot(plt)    
   
    st.subheader("Top 10 nhÃ  hÃ ng cÃ³ sá»‘ lÆ°á»£ng Ä‘Ã¡nh giÃ¡ nhiá»u nháº¥t")
    plt.figure(figsize=(10, 8))
    axtop10 = top_10.plot(kind='barh')
    axtop10.set_xlabel("Number of Reviews")
    plt.title('Top 10 Restaurants with Most Reviews')
    for container in axtop10.containers:
        axtop10.bar_label(container, label_type='edge')
    st.pyplot(plt)    
 
    df_plot_sent = data['Rating_Score'].value_counts()
    st.subheader("PhÃ¢n phá»‘i Rating")
    fig, ax = plt.subplots()
    ax.pie(df_plot_sent, labels=df_plot_sent.index, autopct='%1.1f%%', startangle=90,colors=['#66b3ff', '#ff9999', '#99ff99'])
    ax.axis('equal') 
    st.pyplot(fig)

elif choice == 'Model':
    st.title("ğŸ“ˆ Model")
    st.markdown("##")
    left_column5, middle_column5, right_column5 = st.columns(3)
    with left_column5:
        st.subheader('NaiveBayes')
        st.image('NB.JPG')
    with middle_column5:
        st.subheader('Logistic Regression')
        st.image('LR.JPG')
    with right_column5:
        st.subheader('Dicission Tree')
        st.image('DT.JPG')
    st.markdown('---')

    left_column6, middle_column6, right_column6 = st.columns(3)
    with left_column6:
        st.subheader('NaiveBayes-OverRandomSampler')
        st.image('NB-O.JPG')
    with middle_column6:
        st.subheader('Logistic Regression-OverRandomSampler')
        st.image('LR-O.JPG')
    with right_column6:
        st.subheader('Dicission Tree-OverRandomSampler')
        st.image('DT-O.JPG')
    st.subheader('- Qua cÃ¡c káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p Test tháº¥y Ä‘Æ°á»£c mÃ´ hÃ¬nh:')
    st.subheader('- LR_Over_2 (Logistic Regression vá»›i phÆ°Æ¡ng phÃ¡p Over-sampling (RandomOverSampling)) Ä‘em láº¡i káº¿t quáº£ tá»‘t nháº¥t')
    st.subheader('- Chá»‰ sá»‘ Recall, Precision Ä‘á»u khÃ¡ cao (0.7 ~ 0.8), F1-Score trÃªn 0.8, AUC Ä‘áº¡t 0.8976')
    st.subheader('- HÆ¡n ná»¯a so sÃ¡nh trá»±c quan Cofusion Matrix cho tháº¥y nhÃ£n Positive vÃ  Negative dá»± Ä‘oÃ¡n Ä‘Æ°á»£c tá»‘t nháº¥t trong táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh trÃªn')
    st.subheader('- Do Ä‘Ã³ ta sáº½ chá»n MÃ´ hÃ¬nh Logistic Regression RandomOverSampling Ä‘á»ƒ dá»± Ä‘oÃ¡n trÃªn toÃ n bá»™ dá»¯ liá»‡u')



elif choice == 'Sentiment Analysis':
    st.title("ğŸ™‚ğŸ˜ğŸ˜  Predict Sentiment")
    st.markdown("##")
    st.subheader("Text")
    with st.form(key='nlpForm'):
        text = st.text_area("Enter Text Here")
        submit_button = st.form_submit_button(label = 'Analyze')
    col1, col2 = st.columns(2)
    if submit_button:
        with col1:
            st.info("Result")
            x_new = clean_text_str(text) 
            if isinstance(x_new, str):
                x_new = [x_new]
            x_new = tfidf_model.transform(x_new)        
            y_pred_new = sentiment_model.predict(x_new)       
            st.write(y_pred_new)
            if y_pred_new == 1:
                st.markdown("Positive :smiley:")
            else:
                st.markdown("Negative :angry:")
        with col2:
            st.info ("Sentiment")
            if y_pred_new == 1:
                st.image("smile.png")
            else:
                st.image("sad.png")

    st.subheader("Upload File")
    with st.form(key='dfform'):
        # Upload file
        uploaded_file = st.file_uploader("Choose a file", type=['xlsx'])
        submit_button = st.form_submit_button(label = 'Analyze')

        if uploaded_file is not None:
            st.markdown('---')
            df = pd.read_excel(uploaded_file, header=None, engine='openpyxl')
            st.markdown('Users comments')
            st.dataframe(df)
            # st.write(lines.columns)
            lines = df.iloc[:, 0]    
            if len(lines)>0:
                cleaned_lines = [clean_text_str(str(line)) for line in lines]      
                x_new = tfidf_model.transform(lines)        
                y_pred_new = sentiment_model.predict(x_new)
                df['Sentiment'] = y_pred_new
                df['Content Emoji'] = [predict_sentiment(text) for text in y_pred_new]
                st.markdown('Prediction')
                st.dataframe(df)       


elif  choice == 'Restaurant Information':
    data_res = pd.read_csv('df_restaurants_fn.csv')
    res = st.multiselect(
                        "Select the Restaurant:", 
                        options = data_res["Restaurant"].unique(),
                        max_selections = 1)
    df_selection = data_res.query("Restaurant == @res")
    if df_selection.empty:
        st.warning("No data available based on the current filter settings!")

    st.title(":bar_chart: Restaurant Info")
    st.markdown("##")

    name = df_selection["Restaurant"].values[0]
    rating_score = df_selection["Sentiment"].values[0]
    rating =  ":star:" * int(round(df_selection["Rating"].values[0], 0))
    star_rating = round( df_selection["Rating"].values[0], 1)
    price = df_selection["Price"].values[0]
    pos = df_selection["Positive"].values[0]
    neg = df_selection["Negative"].values[0]
    neu = df_selection["Neutral"].values[0]
    dis = df_selection["District"].values[0]
    add = df_selection["Address"].values[0]
    max = df_selection["Most_Reviewed_Hour"].values[0]
    min = df_selection["Min_Reviewed_Hour"].values[0]


    left_column, middle_column, right_column = st.columns(3)
    with left_column:
        st.subheader("ğŸ½ï¸Name:")
        st.subheader(f"{name:}")
    with middle_column:
        st.subheader("ğŸ“Address:")
        st.subheader(f"{add:}")
    with right_column:
        st.subheader("ğŸ—ºï¸District:")
        st.subheader(f"{dis}")
    st.markdown("""---""")


    left_column1, middle_column1, right_column1 = st.columns(3)
    with left_column1:
        st.subheader("ğŸ“Rating:")
        st.subheader((f"{star_rating} {rating}"))
    with middle_column1:
        st.subheader("â¤ï¸Sentiment:")
        st.subheader(f"{rating_score:}")
    with right_column1:
        st.subheader("ğŸ·ï¸Price:")
        st.subheader(f"{price:}")
    st.markdown("""---""")

    left_column2, middle_column2, right_column2 = st.columns(3)
    with left_column2:
        st.subheader("â˜¹ï¸Negative:")
        st.subheader(f"{neg:}")
    with middle_column2:
        st.subheader("ğŸ˜ŠPositive:")
        st.subheader(f"{pos:}")
    with right_column2:
        st.subheader("ğŸ˜Neutral:")
        st.subheader(f"{neu:}")
    st.markdown("""---""")

    left_column3, middle_column3, right_column3 = st.columns(3)
    with right_column3:
        st.subheader("ğŸ”¼Most Review Hour:")
        st.subheader(f"{max:}")
    with middle_column3:
        st.subheader("ğŸ”½Min Review Hour:")
        st.subheader(f"{min:}")
    with left_column3:
        df_plot_sent = df_selection.groupby(['Restaurant']).sum()[['Positive', 'Negative', 'Neutral']]
        st.subheader("Sentiment Distribution")
        for restaurant, row in df_plot_sent.iterrows():
            plt.figure(figsize=(2, 2))
            plt.pie(row, labels=row.index, autopct='%1.1f%%', startangle=90, colors=['#66b3ff', '#ff9999', '#99ff99'], textprops={'fontsize': 8})
            st.pyplot(plt)
    st.markdown("""---""")

    left_column4, right_column4 = st.columns(2)
    with left_column4:
        st.subheader("Positive Comments")
        pos_text = df_selection["comment_positive"].values[0]
        pw = WordCloud(width=400, height=200, background_color='white').generate(pos_text)
        plt.figure(figsize=(10, 5))
        plt.imshow(pw, interpolation='bilinear')
        plt.axis('off')
        st.pyplot(plt)
    with right_column4:
        st.subheader("Negative Comments")
        neg_text = df_selection["comment_negative"].values[0]
        nw = WordCloud(width=400, height=200, background_color='white').generate(neg_text)
        plt.figure(figsize=(10, 5))
        plt.imshow(nw, interpolation='bilinear')
        plt.axis('off')
        st.pyplot(plt)




            
                




        


           
